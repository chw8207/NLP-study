{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1crYgN6HZpEq04YX5j6cayU4NbKrdQMCU",
      "authorship_tag": "ABX9TyMhy3HmEZ1bUz2TDggjg93j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chw8207/NLP-study/blob/main/%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPBPDaQpbmAr",
        "outputId": "019f8afb-da27-4a2f-a76c-82d485f30eed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.10/dist-packages (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from annoy import AnnoyIndex\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lMHnJ6QRpdYS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 훈련된 단어 임베딩 사용하기\n",
        "class PreTrainedEmbeddings(object) : \n",
        "  def __init__(self, word_to_index, word_vectors) : \n",
        "      \"\"\"\n",
        "        매개변수:\n",
        "            word_to_index (dict): 단어에서 정수로 매핑\n",
        "            word_vectors (numpy 배열의 리스트)\n",
        "      \"\"\"\n",
        "      self.word_to_index = word_to_index\n",
        "      self.word_vectors = word_vectors\n",
        "      self.index_to_word = {v:k for k, v in self.word_to_index.items()}\n",
        "      # AnnoyIndex(len(word_vectors[0]), metric='거리척도')\n",
        "      # 근사 최근접 이웃 탐색을 제공함\n",
        "      # len(word_vectors[0]) : 첫 번째 단어 벡터의 길이 활용 \n",
        "      #                        모든 단어의 벡터 길이가 동일해야하기 때문\n",
        "      self.index = AnnoyIndex(f=100, metric='euclidean') \n",
        "      print(\"인덱스 만드는 중!\")\n",
        "      # i = 0\n",
        "      for _,i in word_to_index.items() : \n",
        "        self.index.add_item(i, word_vectors[i])\n",
        "      self.index.build(50)\n",
        "      print(\"완료!\")\n",
        "\n",
        "  @classmethod\n",
        "  def from_embeddings_file(cls, embedding_file) : \n",
        "    \"\"\"사전 훈련된 벡터 파일에서 객체를 만듭니다.\n",
        "        \n",
        "        벡터 파일은 다음과 같은 포맷입니다:\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
        "        \n",
        "        매개변수:\n",
        "            embedding_file (str): 파일 위치\n",
        "        반환값:\n",
        "            PretrainedEmbeddings의 인스턴스\n",
        "    \"\"\"\n",
        "    word_to_index = {}\n",
        "    word_vectors = []\n",
        "    with open(embedding_file) as fp : \n",
        "      for line in fp.readlines() : \n",
        "        line = line.split(' ')\n",
        "        word = line[0]\n",
        "        # 리스트의 나머지 요소들을 실수로 변환 후\n",
        "        # 넘파이 배열로 변환 후 벡터를 생성함\n",
        "        vec = np.array([float(x) for x in line[1:]])\n",
        "        # 해당 단어가 주어지면 이 단어를 딕셔너리의 키로 사용\n",
        "        # 딕셔너리의 현재 값을 해당 단어의 인덱스로 설정\n",
        "        word_to_index[word] = len(word_to_index)\n",
        "        word_vectors.append(vec)\n",
        "    return cls(word_to_index, word_vectors)\n",
        "\n",
        "# 단어 임베딩을 활용한 유추 작업\n",
        "  def get_embedding(self, word) : \n",
        "    \"\"\"\n",
        "        매개변수:\n",
        "            word (str)\n",
        "        반환값\n",
        "            임베딩 (numpy.ndarray)\n",
        "    \"\"\"\n",
        "    return self.word_vectors[self.word_to_index]\n",
        "\n",
        "  def get_closets_to_vector(self, vector, n=1) : \n",
        "    \"\"\"벡터가 주어지면 n 개의 최근접 이웃을 반환합니다\n",
        "        매개변수:\n",
        "            vector (np.ndarray): Annoy 인덱스에 있는 벡터의 크기와 같아야 합니다\n",
        "            n (int): 반환될 이웃의 개수\n",
        "        반환값:\n",
        "            [str, str, ...]: 주어진 벡터와 가장 가까운 단어\n",
        "                단어는 거리순으로 정렬되어 있지 않습니다.\n",
        "    \"\"\"\n",
        "    nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "    return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "\n",
        "  def compute_and_print_analogy(self, word1, word2, word3) : \n",
        "    \"\"\"단어 임베딩을 사용한 유추 결과를 출력합니다\n",
        "\n",
        "        word1이 word2일 때 word3은 __입니다.\n",
        "        이 메서드는 word1 : word2 :: word3 : word4를 출력합니다\n",
        "        \n",
        "        매개변수:\n",
        "            word1 (str)\n",
        "            word2 (str)\n",
        "            word3 (str)\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "KAaSSh-EsGAn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OQpSuRlzVjx",
        "outputId": "1931a002-edb0-4086-a2b4-6cfe6b2982ed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-09 05:06:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-06-09 05:06:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-06-09 05:06:58--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        0%[                    ]       0  --.-KB/s               ^C\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = PreTrainedEmbeddings.from_embeddings_file('/content/drive/MyDrive/Colab Notebooks/자연어처리공부/NLP-study/NLP-study/data/glove/glove.6B.100d.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSj5KnmlylQX",
        "outputId": "3d2448b6-72a3-4fb2-ec0c-ffda80c19664"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 만드는 중!\n",
            "완료!\n"
          ]
        }
      ]
    }
  ]
}